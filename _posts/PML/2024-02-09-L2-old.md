
Regression algorithms -> predict a continuous numerical value

Classification algorithms -> predict discrete outputs

typical format of data:
- $X_{i}$ = raw information of the sample
- $Y_{i}$ = regression target value

A collection of data sample( X ) = a dataset.

The input and output of regression can be multi-dimensional.

    Is there any example with a single-dimensional input and multi-dimensional output?

# Tradition Types of Regression Models
- Linear Regression
    - Simple Linear Regression
    - Multiple Linear Regression
- Polynomial Regression
- Regularization Methods
    - Ridge Regression (L2 Regularization)
    - Lasso Regression (L1 Regularization)
- Logistic Regression

    Universal Approximation Theorem: A feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function

# Simple Linear Regression
Simple linear regression attempts to find the best-fitting straight line through these points.
## SSR = Sum of squared residuals 

# Multiple Linear Regression
multiple linear regression attempts to find the best-fitting straight plane through data points.

# Polynomial Regression
Polynomial regression models the relationship between the independent variable,Â ğ‘¥, and the dependent variable (observation),Â ğ‘¦, as anÂ ğ‘›-th degree polynomial. 

    Higher-degree polynomials can lead to overfitting
    Choose the appropriate degree of polynomials by
        Cross-Validation
        Regularization

# Ridge Regression (L2 Regularization)
deal with overfitting\
The model is too complex\
The data samples may not be independent

The L2 penalty encourages smaller, more diffuse coefficient values, leading to a model that is less likely to overfit to the training data.

Î»: Tuning parameter that controls the strength of the penalty term.
# Lasso Regression (L1 Regularization)
    some factors (e.g., age of owner) are not useful! In many cases, we have designed â€œover-complicatedâ€ models by considering all different factors, which will lead to overfitting.

L1 norm generates sparser solutions!

If sparse solutions can be obtained for ğœ·, we can identify the really useful factors, and make the weights to non-useful factors to zero.

# Logistic Regression
    The Logistic Regression can be used to predict these continuous probability values, while the target follows Bernoulli distribution.
